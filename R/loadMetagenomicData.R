#' Rewriting of loadMetagenomicData in parkinsonsMetagenomicData
#'
#' This function should allegedly work faster and be more flexible toward
#' empty files. Additionally, it makes use of mia::importMetaphlan, which 
#' is constantly maintained tested. **IMPORTANT**: to work, the function must
#' have the right to write in its temporary directory. This code may not work in
#' some cloud computing environments.
#'
#' @param cache_table \code{data.frame} containing local paths where data have been 
#' cached with `cacheMetagenomicData`
#'
#' @returns A \code{TreeSummarizedExperiment} (without phylogenetic tree), as 
#' generated by `mia:importMetaPhlAn`.
#' 
#' @importFrom dplyr select mutate rename filter
#' @importFrom tidyr pivot_wider
#' @import parkinsonsMetagenomicData
#' @export
#'
#' @examples
#' 
#' # careful, this example takes a few minutes to run
#' \dontrun{
#' library(parkinsonsMetagenomicData)
#' 
#' human_parkinson_data <- dplyr::filter(sampleMetadata, !grepl("Mazmanian", study_name))
#' 
#' cached_profiles <- suppressMessages(cacheMetagenomicData(human_parkinson_data$uuid, data_type = "relative_abundance"))
#' 
#' is_downloaded <- sapply(cached_profiles$cache_path, file.exists)
#' 
#' table(is_downloaded)
#' 
#' cache_table <- cached_profiles[is_downloaded,]
#' 
#' # this one fails with this subset of samples
#' data.tse <- loadMetagenomicData(cache_table)
#' 
#' # this one does not fail
#' data.tse <- loadMetagenomicData(cache_table)
#' }

loadMetagenomicData <- function(cache_table){
  
  files_to_read <- cache_table$cache_path
  names(files_to_read) <- cache_table$UUID
  
  runInfo <- getMetaPhlAn_run_info(files_to_read)
  # make the colData
  colData.df <- dplyr::filter(parkinsonsMetagenomicData::sampleMetadata, uuid %in% cache_table$UUID)
  colData.df <- colData.df[match(names(runInfo[["reads_processed"]]), colData.df$uuid),]
  colData.df[["number_reads"]] <- runInfo[["reads_processed"]]
  rownames(colData.df) <- colData.df[["uuid"]]
  
  # find where profiles start (not too necessary, but it's a more flexible check)
  example_line <- readLines(files_to_read[[1]], n = 8)
  line_to_read_from <- grep("\\#clade_name|\\#ID", example_line)
  
  # define the columns to keep, hopefully this is compatible with metaphlan3, but it is not tested yet
  cols_all <- str_split(example_line[line_to_read_from], pattern = "\\t")[[1]]
  cols_to_keep <- cols_all[grepl("clade|abund|count", tolower(cols_all))]
  
  # read file as raw input 
  input_raw.tb <- read_tsv(files_to_read, skip = 4, progress = FALSE, id = "fileName") |>
    # add uuid codes instead of cache
    dplyr::mutate(uuid = names(files_to_read)[match(fileName, files_to_read)]) |>
    # keep only columns we are interested in
    dplyr::select(all_of(c(cols_to_keep, "uuid")))
  # remove hashtag from first column name
  colnames(input_raw.tb) <- gsub("#", "", colnames(input_raw.tb), fixed = TRUE)
  
  
  mia_version <- as.integer(str_split(package.version("mia"), "\\.", simplify = TRUE))
  if(mia_version[1] == 1 &
     mia_version[2] <= 16) {
  # complete UNCLASSIFIED with a fake taxonomy, necessary only until mia doesn't 
  # push my feature request into Bioconductor
  tax_lvl_int <- max(str_count(input_raw.tb$clade_name, pattern = "\\|")) + 1
  input_raw.tb$clade_name[input_raw.tb$clade_name == "UNCLASSIFIED"] <- paste(names(all_taxonomy_levels)[1:tax_lvl_int],  "UNCLASSIFIED", collapse = "|", sep = "")
  }
  
  input_pivoted <- pivot_wider(data = input_raw.tb, names_from = "uuid", values_from = cols_to_keep[2], values_fill = 0)
  tmpFile <- file.path(tempdir(), "profiles.tsv")
  write_tsv(input_pivoted, tmpFile)
  
  data.tse <- mia::importMetaPhlAn(tmpFile, col.data = colData.df)
  
  S4Vectors::metadata(data.tse)[["MetaPhlAn_run_info"]] <- runInfo[names(runInfo) != "reads_processed"]
  
  return(data.tse)
}


#' Extract run info of a MetaPhlAn run 
#'
#' @param files_to_read A \code{character} vector with the metaphlan profile already 
#' read into R with readLines
#'
#' @returns A \code{list} of MetaPhlAn run information (db version, code, reads)
#' @export
#' 
#' @importFrom readr parse_number
#'
#' @examples
#' 
#' \dontrun{
#' mpaProfile <- readLines("path/to/metaphlan_run.tsv")
#' 
#' mpa_run_info <- getMetaPhlAn_run_info(mpaProfile)
#' 
#' }
#' 
getMetaPhlAn_run_info <- function(files_to_read){
  
  # read first 8 lines of all files
  fileStats_headers <- lapply(files_to_read, function(filePath) readLines(filePath, 8))
  # extract metaphlan run information first N lines
  chocophlan_version <- unique(sapply(fileStats_headers, function(x) gsub("#" , "", x[1])))
  metaphlan_run_command <- unique(sapply(fileStats_headers, function(x) gsub("#" , "", x[2])))
  reads_processed <- sapply(fileStats_headers, function(x) readr::parse_number(x[3]))
  # check that the versions are uniform
  if(length(chocophlan_version) != 1){
    stop("Files were not run with one single CHOCOPhlAn database version")
  } 
  
  if(length(metaphlan_run_command) != 1){
    stop("Files were not run with one single metaphlan command")
  }
  
  return(
    list(
      "CHOCOPhlAn_version" = chocophlan_version,
      "MetaPhlAn command" = metaphlan_run_command,
      "reads_processed" = reads_processed
    )
  )
  
}

